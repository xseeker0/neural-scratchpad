{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPu2KpfDycQM0SAgXMvqENN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xseeker0/neural-scratchpad/blob/main/practice_2_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural net"
      ],
      "metadata": {
        "id": "H4OrvlYeN2yf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer"
      ],
      "metadata": {
        "id": "tm01C7_TOIgq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LQIOnbRy6bkb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MyLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        # He Initialization\n",
        "        self.weight = nn.Parameter(torch.randn(in_features, out_features) * math.sqrt(2. / in_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # y = xW + b\n",
        "        return torch.matmul(x, self.weight) + self.bias\n",
        "\n",
        "class MyFlatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        # Keep batch size (dim 0) and flatten all other dimensions\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class Sigmoid(nn.Module):\n",
        "    def forward(self, x):\n",
        "        # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
        "        # Maps input values to the range (0, 1)\n",
        "        return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "class MyReLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        # f(x) = max(0, x)\n",
        "        return torch.clamp(x, min=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "n_s-ycLsOjTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MySGD:\n",
        "    def __init__(self, params, lr):\n",
        "        \"\"\"\n",
        "        Initialize the optimizer.\n",
        "        params: Iterable of parameters to optimize (typically model.parameters())\n",
        "        lr: Learning rate for the update rule\n",
        "        \"\"\"\n",
        "        # Store parameters as a list to ensure they can be iterated multiple times\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step (parameter update).\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            for param in self.params:\n",
        "                if param.grad is not None:\n",
        "                    # Apply the SGD update rule: w = w - lr * grad\n",
        "                    param.data -= self.lr * param.grad\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Resets the gradients of all optimized parameters to zero.\n",
        "        \"\"\"\n",
        "        for param in self.params:\n",
        "            if param.grad is not None:\n",
        "                # Use in-place zeroing to clear the gradient buffer\n",
        "                param.grad.zero_()"
      ],
      "metadata": {
        "id": "H_KEHJYoOmy2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost (Loss function)\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "2GBUsGHJOnST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        logits: (batch_size, num_classes) - Raw unnormalized scores from the model\n",
        "        targets: (batch_size) - Ground truth class indices (not one-hot encoded)\n",
        "        \"\"\"\n",
        "        # 1. Find max value for numerical stability (Log-Sum-Exp trick)\n",
        "        max_logits = torch.max(logits, dim=1, keepdim=True)[0]\n",
        "\n",
        "        # 2. Compute Log-Sum-Exp: log(sum(exp(x - max))) + max\n",
        "        log_sum_exp = torch.log(torch.sum(torch.exp(logits - max_logits), dim=1, keepdim=True)) + max_logits\n",
        "\n",
        "        # 3. Extract the logits of the correct classes\n",
        "        # targets.view(-1, 1) allows selecting the correct index for each batch element\n",
        "        correct_class_logits = torch.gather(logits, 1, targets.view(-1, 1))\n",
        "\n",
        "        # 4. Cross Entropy Formula: Loss = - (logit_correct - log_sum_exp)\n",
        "        loss = - (correct_class_logits - log_sum_exp)\n",
        "\n",
        "        # Return the mean loss over the batch\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "9HJSzOKvOtf3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "5AB0dFMROvLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data"
      ],
      "metadata": {
        "id": "HT4YZrmIXDkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTw3MFIlXJQ2",
        "outputId": "349f05f0-7881-461b-c9f3-1a902e817a3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 13.7MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 205kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.78MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 23.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model architect"
      ],
      "metadata": {
        "id": "keh39qX7XKV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = MyFlatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            MyLinear(28*28, 512),\n",
        "            MyReLU(),\n",
        "            MyLinear(512, 512),\n",
        "            MyReLU(),\n",
        "            MyLinear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = MyNeuralNetwork()\n",
        "\n",
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVvEhQf0OxlI",
        "outputId": "168a4d90-6715-4f9f-baa8-3d4bced53fa7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: MyNeuralNetwork(\n",
            "  (flatten): MyFlatten()\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): MyLinear()\n",
            "    (1): MyReLU()\n",
            "    (2): MyLinear()\n",
            "    (3): MyReLU()\n",
            "    (4): MyLinear()\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([784, 512]) | Values : tensor([[ 0.0010,  0.0075, -0.0328,  ..., -0.0225, -0.0315, -0.0003],\n",
            "        [ 0.0187, -0.0158, -0.0365,  ..., -0.0142, -0.0211,  0.0164]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.1204,  0.0277, -0.1255,  ...,  0.0430,  0.0120,  0.0211],\n",
            "        [ 0.0070,  0.0304, -0.0323,  ...,  0.0141,  0.0487, -0.0584]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([512, 10]) | Values : tensor([[-0.0457,  0.0069, -0.0263, -0.0234, -0.0065,  0.0364,  0.0698, -0.0147,\n",
            "          0.0866,  0.0462],\n",
            "        [-0.0368,  0.0072,  0.1053, -0.0719,  0.0293, -0.0069,  0.0218,  0.0731,\n",
            "          0.0584, -0.0344]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "Xw-Lh1lgOyDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "\n",
        "optimizer = MySGD(model.parameters(), lr=learning_rate)\n",
        "loss_fn = MyCrossEntropyLoss()\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "zMdAip40O2rh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny2JsjhWYbeV",
        "outputId": "16eed91d-fba9-4b22-cc78-4880063e2e3d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.363343  [   64/60000]\n",
            "loss: 1.984593  [ 6464/60000]\n",
            "loss: 1.692523  [12864/60000]\n",
            "loss: 1.621384  [19264/60000]\n",
            "loss: 1.344239  [25664/60000]\n",
            "loss: 1.286727  [32064/60000]\n",
            "loss: 1.247081  [38464/60000]\n",
            "loss: 1.072921  [44864/60000]\n",
            "loss: 1.159184  [51264/60000]\n",
            "loss: 1.040135  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.9%, Avg loss: 1.034097 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.058667  [   64/60000]\n",
            "loss: 1.082028  [ 6464/60000]\n",
            "loss: 0.861875  [12864/60000]\n",
            "loss: 1.050691  [19264/60000]\n",
            "loss: 0.878197  [25664/60000]\n",
            "loss: 0.907323  [32064/60000]\n",
            "loss: 0.918123  [38464/60000]\n",
            "loss: 0.809571  [44864/60000]\n",
            "loss: 0.912612  [51264/60000]\n",
            "loss: 0.835164  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.3%, Avg loss: 0.825587 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.811993  [   64/60000]\n",
            "loss: 0.896856  [ 6464/60000]\n",
            "loss: 0.672359  [12864/60000]\n",
            "loss: 0.898766  [19264/60000]\n",
            "loss: 0.758653  [25664/60000]\n",
            "loss: 0.781958  [32064/60000]\n",
            "loss: 0.805169  [38464/60000]\n",
            "loss: 0.721098  [44864/60000]\n",
            "loss: 0.822391  [51264/60000]\n",
            "loss: 0.746898  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.6%, Avg loss: 0.734256 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.698853  [   64/60000]\n",
            "loss: 0.802131  [ 6464/60000]\n",
            "loss: 0.582666  [12864/60000]\n",
            "loss: 0.820805  [19264/60000]\n",
            "loss: 0.696399  [25664/60000]\n",
            "loss: 0.711939  [32064/60000]\n",
            "loss: 0.740292  [38464/60000]\n",
            "loss: 0.677357  [44864/60000]\n",
            "loss: 0.776710  [51264/60000]\n",
            "loss: 0.687441  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.3%, Avg loss: 0.678816 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.631192  [   64/60000]\n",
            "loss: 0.737917  [ 6464/60000]\n",
            "loss: 0.528057  [12864/60000]\n",
            "loss: 0.771127  [19264/60000]\n",
            "loss: 0.655019  [25664/60000]\n",
            "loss: 0.665522  [32064/60000]\n",
            "loss: 0.694606  [38464/60000]\n",
            "loss: 0.653772  [44864/60000]\n",
            "loss: 0.749162  [51264/60000]\n",
            "loss: 0.641767  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.2%, Avg loss: 0.640487 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.584506  [   64/60000]\n",
            "loss: 0.690199  [ 6464/60000]\n",
            "loss: 0.490199  [12864/60000]\n",
            "loss: 0.735296  [19264/60000]\n",
            "loss: 0.624754  [25664/60000]\n",
            "loss: 0.633434  [32064/60000]\n",
            "loss: 0.660342  [38464/60000]\n",
            "loss: 0.640174  [44864/60000]\n",
            "loss: 0.730786  [51264/60000]\n",
            "loss: 0.603565  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.9%, Avg loss: 0.612234 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.550353  [   64/60000]\n",
            "loss: 0.653829  [ 6464/60000]\n",
            "loss: 0.461655  [12864/60000]\n",
            "loss: 0.708077  [19264/60000]\n",
            "loss: 0.601276  [25664/60000]\n",
            "loss: 0.609408  [32064/60000]\n",
            "loss: 0.632121  [38464/60000]\n",
            "loss: 0.632275  [44864/60000]\n",
            "loss: 0.717416  [51264/60000]\n",
            "loss: 0.573343  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.4%, Avg loss: 0.590542 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.523284  [   64/60000]\n",
            "loss: 0.625293  [ 6464/60000]\n",
            "loss: 0.439546  [12864/60000]\n",
            "loss: 0.686228  [19264/60000]\n",
            "loss: 0.583121  [25664/60000]\n",
            "loss: 0.590246  [32064/60000]\n",
            "loss: 0.609818  [38464/60000]\n",
            "loss: 0.627119  [44864/60000]\n",
            "loss: 0.706949  [51264/60000]\n",
            "loss: 0.548834  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.573187 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.500807  [   64/60000]\n",
            "loss: 0.601960  [ 6464/60000]\n",
            "loss: 0.420944  [12864/60000]\n",
            "loss: 0.667831  [19264/60000]\n",
            "loss: 0.567064  [25664/60000]\n",
            "loss: 0.574107  [32064/60000]\n",
            "loss: 0.591066  [38464/60000]\n",
            "loss: 0.623946  [44864/60000]\n",
            "loss: 0.699126  [51264/60000]\n",
            "loss: 0.527703  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.3%, Avg loss: 0.558819 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.482069  [   64/60000]\n",
            "loss: 0.583220  [ 6464/60000]\n",
            "loss: 0.405429  [12864/60000]\n",
            "loss: 0.651675  [19264/60000]\n",
            "loss: 0.553314  [25664/60000]\n",
            "loss: 0.560074  [32064/60000]\n",
            "loss: 0.575467  [38464/60000]\n",
            "loss: 0.622247  [44864/60000]\n",
            "loss: 0.692067  [51264/60000]\n",
            "loss: 0.510037  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.546713 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use pytorch layer, optimizer and loss"
      ],
      "metadata": {
        "id": "kBtzwSO_PlEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()\n",
        "\n",
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zR7BzL9PnS_",
        "outputId": "580b529b-a11f-49ce-bcd5-f661f17b96fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0336,  0.0010, -0.0282,  ..., -0.0230,  0.0194, -0.0216],\n",
            "        [ 0.0156, -0.0045, -0.0016,  ...,  0.0042,  0.0183,  0.0251]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0351, -0.0274], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0362, -0.0198, -0.0326,  ..., -0.0397, -0.0109,  0.0270],\n",
            "        [ 0.0272, -0.0199,  0.0218,  ..., -0.0147,  0.0124,  0.0391]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0437, -0.0284], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0165,  0.0293, -0.0267,  ...,  0.0389,  0.0124,  0.0396],\n",
            "        [-0.0316,  0.0372,  0.0266,  ..., -0.0224, -0.0270, -0.0381]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0014,  0.0247], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.314528  [   64/60000]\n",
            "loss: 2.300541  [ 6464/60000]\n",
            "loss: 2.286093  [12864/60000]\n",
            "loss: 2.276373  [19264/60000]\n",
            "loss: 2.249499  [25664/60000]\n",
            "loss: 2.228183  [32064/60000]\n",
            "loss: 2.229267  [38464/60000]\n",
            "loss: 2.202645  [44864/60000]\n",
            "loss: 2.196806  [51264/60000]\n",
            "loss: 2.156718  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.0%, Avg loss: 2.158287 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.178517  [   64/60000]\n",
            "loss: 2.163554  [ 6464/60000]\n",
            "loss: 2.111303  [12864/60000]\n",
            "loss: 2.118547  [19264/60000]\n",
            "loss: 2.068949  [25664/60000]\n",
            "loss: 2.021010  [32064/60000]\n",
            "loss: 2.036354  [38464/60000]\n",
            "loss: 1.966532  [44864/60000]\n",
            "loss: 1.959832  [51264/60000]\n",
            "loss: 1.880314  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 54.8%, Avg loss: 1.885993 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.934213  [   64/60000]\n",
            "loss: 1.896970  [ 6464/60000]\n",
            "loss: 1.780107  [12864/60000]\n",
            "loss: 1.808158  [19264/60000]\n",
            "loss: 1.708422  [25664/60000]\n",
            "loss: 1.662307  [32064/60000]\n",
            "loss: 1.676939  [38464/60000]\n",
            "loss: 1.579949  [44864/60000]\n",
            "loss: 1.592697  [51264/60000]\n",
            "loss: 1.488999  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.2%, Avg loss: 1.511397 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.589505  [   64/60000]\n",
            "loss: 1.548651  [ 6464/60000]\n",
            "loss: 1.395952  [12864/60000]\n",
            "loss: 1.467844  [19264/60000]\n",
            "loss: 1.358337  [25664/60000]\n",
            "loss: 1.348435  [32064/60000]\n",
            "loss: 1.371201  [38464/60000]\n",
            "loss: 1.289542  [44864/60000]\n",
            "loss: 1.316722  [51264/60000]\n",
            "loss: 1.223529  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.3%, Avg loss: 1.250414 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.334793  [   64/60000]\n",
            "loss: 1.310336  [ 6464/60000]\n",
            "loss: 1.142414  [12864/60000]\n",
            "loss: 1.250993  [19264/60000]\n",
            "loss: 1.134395  [25664/60000]\n",
            "loss: 1.151330  [32064/60000]\n",
            "loss: 1.184514  [38464/60000]\n",
            "loss: 1.113430  [44864/60000]\n",
            "loss: 1.146879  [51264/60000]\n",
            "loss: 1.064432  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.8%, Avg loss: 1.087831 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.165614  [   64/60000]\n",
            "loss: 1.160279  [ 6464/60000]\n",
            "loss: 0.975512  [12864/60000]\n",
            "loss: 1.112715  [19264/60000]\n",
            "loss: 0.993566  [25664/60000]\n",
            "loss: 1.018119  [32064/60000]\n",
            "loss: 1.065268  [38464/60000]\n",
            "loss: 1.000147  [44864/60000]\n",
            "loss: 1.035671  [51264/60000]\n",
            "loss: 0.961553  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 0.981112 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.046422  [   64/60000]\n",
            "loss: 1.061183  [ 6464/60000]\n",
            "loss: 0.859879  [12864/60000]\n",
            "loss: 1.019627  [19264/60000]\n",
            "loss: 0.904310  [25664/60000]\n",
            "loss: 0.923438  [32064/60000]\n",
            "loss: 0.985958  [38464/60000]\n",
            "loss: 0.925867  [44864/60000]\n",
            "loss: 0.958866  [51264/60000]\n",
            "loss: 0.892106  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.0%, Avg loss: 0.908067 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.958323  [   64/60000]\n",
            "loss: 0.991956  [ 6464/60000]\n",
            "loss: 0.777017  [12864/60000]\n",
            "loss: 0.954462  [19264/60000]\n",
            "loss: 0.845888  [25664/60000]\n",
            "loss: 0.854008  [32064/60000]\n",
            "loss: 0.930431  [38464/60000]\n",
            "loss: 0.876016  [44864/60000]\n",
            "loss: 0.903530  [51264/60000]\n",
            "loss: 0.842357  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.3%, Avg loss: 0.855686 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.890784  [   64/60000]\n",
            "loss: 0.940112  [ 6464/60000]\n",
            "loss: 0.715345  [12864/60000]\n",
            "loss: 0.906555  [19264/60000]\n",
            "loss: 0.805010  [25664/60000]\n",
            "loss: 0.801829  [32064/60000]\n",
            "loss: 0.888814  [38464/60000]\n",
            "loss: 0.841119  [44864/60000]\n",
            "loss: 0.861702  [51264/60000]\n",
            "loss: 0.805003  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.5%, Avg loss: 0.816165 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.836680  [   64/60000]\n",
            "loss: 0.898683  [ 6464/60000]\n",
            "loss: 0.667593  [12864/60000]\n",
            "loss: 0.869989  [19264/60000]\n",
            "loss: 0.774712  [25664/60000]\n",
            "loss: 0.761884  [32064/60000]\n",
            "loss: 0.855621  [38464/60000]\n",
            "loss: 0.815279  [44864/60000]\n",
            "loss: 0.828881  [51264/60000]\n",
            "loss: 0.775302  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.7%, Avg loss: 0.784881 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}